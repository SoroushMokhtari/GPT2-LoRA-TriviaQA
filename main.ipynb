{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f43d7869",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/soroushmokhtari/miniconda3/envs/torch_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import re\n",
    "from joblib import Parallel, delayed\n",
    "import json\n",
    "import random\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "031d0aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Hide the tokenizer warning\n",
    "logging.getLogger(\"transformers.tokenization_utils_base\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f67047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context window size: 1024\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba324578",
   "metadata": {},
   "source": [
    "# Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4431c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d47c6754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context window size: 1024\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model.to(device);\n",
    "\n",
    "context_window_size = model.config.n_ctx\n",
    "print(f'Context window size: {context_window_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695514ed",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924c8dcd",
   "metadata": {},
   "source": [
    "## Chunk Overlap\n",
    "Instead of splitting the text into disjointed pieces, you make each new chunk start a sentence or two before the previous one ended.\n",
    "\n",
    "- Original Context: [S1, S2, S3, S4, S5, S6]\n",
    "- Chunking (no overlap):\n",
    "    - Chunk 1: [S1, S2, S3]\n",
    "    - Chunk 2: [S4, S5, S6]\n",
    "- Chunking (with an overlap of 1 sentence):\n",
    "    - Chunk 1: [S1, S2, S3]\n",
    "    - Chunk 2: [S3, S4, S5]\n",
    "    - Chunk 3: [S5, S6]\n",
    "Why this helps: Let's say S2 introduces \"Judi Dench\" and S3 uses \"she\". With overlap, if a user's query matches the content of S4, you retrieve Chunk 2, which is [S3, S4, S5]. Even though S2 isn't there, the overlapping sentence S3 might still contain enough context to resolve the pronoun \"she\" used within it. It significantly increases the probability that a chunk is self-contained enough for the model to understand.\n",
    "\n",
    "\n",
    "\n",
    "## Other, More Advanced Options:\n",
    "Sentence Windowing: During retrieval, after you find the single best chunk, you also grab the chunk immediately before and after it to create a larger, more context-rich passage for the LLM.\n",
    "Coreference Resolution: The most advanced technique is to use a separate NLP model to perform coreference resolution before chunking. This would process the text and replace pronouns like \"she\" with the noun they refer to (\"Judi Dench\"). This is powerful but adds a lot of complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61bd40d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_context(context, min_context_length=20):\n",
    "    cleanen_context = re.split(r'(?<=[.?!])\\s+', context)\n",
    "    cleanen_context = [x.strip() for x in cleanen_context if len(x) > min_context_length]\n",
    "    return cleanen_context\n",
    "\n",
    "def chunker(sentences, chunk_size=768, overlap_sentences=1):\n",
    "    chunks = []\n",
    "    current_chunk_sentences = []\n",
    "    current_length = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Use add_special_tokens=False to get the pure token length of the sentence\n",
    "        tokenized_sentence = tokenizer.encode(sentence, add_special_tokens=False)\n",
    "        tokenized_length = len(tokenized_sentence)\n",
    "\n",
    "        if tokenized_length > chunk_size:\n",
    "            # If there's a pending chunk, finalize it first\n",
    "            if current_chunk_sentences:\n",
    "                chunks.append(\" \".join(current_chunk_sentences))\n",
    "                current_chunk_sentences = []\n",
    "                current_length = 0\n",
    "            \n",
    "            # Split the oversized sentence's tokens into smaller chunks\n",
    "            for i in range(0, tokenized_length, chunk_size):\n",
    "                sub_chunk_tokens = tokenized_sentence[i : i + chunk_size]\n",
    "                # Decode the sub-chunk back to a string\n",
    "                chunks.append(tokenizer.decode(sub_chunk_tokens))\n",
    "            \n",
    "            # Continue to the next sentence\n",
    "            continue\n",
    "\n",
    "        # If adding the next sentence overflows the chunk, finalize the current one\n",
    "        if current_length + tokenized_length > chunk_size:\n",
    "            chunks.append(\" \".join(current_chunk_sentences))\n",
    "            \n",
    "            # Start new chunk with overlap\n",
    "            if overlap_sentences > 0 and len(current_chunk_sentences) >= overlap_sentences:\n",
    "                current_chunk_sentences = current_chunk_sentences[-overlap_sentences:]\n",
    "            else:\n",
    "                current_chunk_sentences = []\n",
    "            \n",
    "            # Recalculate length of the new chunk (with overlap)\n",
    "            current_length = 0\n",
    "            for s in current_chunk_sentences:\n",
    "                current_length += len(tokenizer.encode(s, add_special_tokens=False))\n",
    "\n",
    "        # Add the sentence to the current chunk\n",
    "        current_chunk_sentences.append(sentence)\n",
    "        current_length += tokenized_length\n",
    "\n",
    "    # Add the last remaining chunk if it exists\n",
    "    if current_chunk_sentences:\n",
    "        chunks.append(\" \".join(current_chunk_sentences))\n",
    "\n",
    "    return chunks\n",
    "    \n",
    "\n",
    "def process_single_context(context, chunk_size, overlap_sentences):\n",
    "    \"\"\"Takes a single context string and returns a list of its chunks.\"\"\"\n",
    "    if not isinstance(context, str):\n",
    "        return [] # Return an empty list for non-string inputs\n",
    "    sentences = split_context(context)\n",
    "    chunks = chunker(sentences, chunk_size=chunk_size, overlap_sentences=overlap_sentences)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24e1807",
   "metadata": {},
   "source": [
    "# Chunking Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24806b26",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 10\u001b[0m\n\u001b[1;32m      5\u001b[0m BATCH_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# --- Streaming Pipeline (Corrected to include Q&A) ---\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Create an iterator that reads the CSV in batches\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m csv_iterator \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(INPUT_CSV_PATH, chunksize\u001b[38;5;241m=\u001b[39mBATCH_SIZE)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(OUTPUT_JSONL_PATH, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m outfile:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting streaming processing to create complete training data source...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "INPUT_CSV_PATH = './Data/movie_trivia_qa.csv'\n",
    "OUTPUT_JSONL_PATH = './Data/processed_chunks_with_qa.jsonl'\n",
    "CHUNK_SIZE_FOR_FUNC = 768\n",
    "OVERLAP_SENTENCES_FOR_FUNC = 1\n",
    "BATCH_SIZE = 500\n",
    "\n",
    "# --- Streaming Pipeline (Corrected to include Q&A) ---\n",
    "\n",
    "# Create an iterator that reads the CSV in batches\n",
    "csv_iterator = pd.read_csv(INPUT_CSV_PATH, chunksize=BATCH_SIZE)\n",
    "\n",
    "with open(OUTPUT_JSONL_PATH, 'w') as outfile:\n",
    "    print(\"Starting streaming processing to create complete training data source...\")\n",
    "    \n",
    "    for i, df_batch in enumerate(csv_iterator):\n",
    "        print(f\"  - Processing batch {i+1}...\")\n",
    "\n",
    "        # Keep track of all relevant columns from the batch\n",
    "        contexts_batch = df_batch['context'].tolist()\n",
    "        questions_batch = df_batch['question'].tolist()\n",
    "        answers_batch = df_batch['answer'].tolist()\n",
    "\n",
    "        # Process only the contexts in parallel to get the chunks\n",
    "        batch_chunk_results = Parallel(n_jobs=-1)(\n",
    "            delayed(process_single_context)(\n",
    "                context,\n",
    "                chunk_size=CHUNK_SIZE_FOR_FUNC,\n",
    "                overlap_sentences=OVERLAP_SENTENCES_FOR_FUNC\n",
    "            ) for context in contexts_batch\n",
    "        )\n",
    "\n",
    "        # Zip everything together and write to the JSONL file\n",
    "        for question, answer, chunks in zip(questions_batch, answers_batch, batch_chunk_results):\n",
    "            output_record = {\n",
    "                'question': question,\n",
    "                'answer': answer,\n",
    "                'chunks': chunks\n",
    "            }\n",
    "            outfile.write(json.dumps(output_record) + '\\n')\n",
    "\n",
    "print(f\"Processing complete. Full data saved to {OUTPUT_JSONL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af177aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading retriever model: all-MiniLM-L6-v2...\n",
      "Model loaded on device: mps\n",
      "Starting retrieval and splitting into training and validation sets...\n",
      "  - Processed 5000 records...\n",
      "  - Processed 10000 records...\n",
      "  - Processed 15000 records...\n",
      "  - Processed 20000 records...\n",
      "  - Processed 25000 records...\n",
      "  - Processed 30000 records...\n",
      "  - Processed 35000 records...\n",
      "  - Processed 40000 records...\n",
      "  - Processed 45000 records...\n",
      "  - Processed 50000 records...\n",
      "  - Processed 55000 records...\n",
      "  - Processed 60000 records...\n",
      "  - Processed 65000 records...\n",
      "  - Processed 70000 records...\n",
      "  - Processed 75000 records...\n",
      "  - Processed 80000 records...\n",
      "  - Processed 85000 records...\n",
      "  - Processed 90000 records...\n",
      "  - Processed 95000 records...\n",
      "  - Processed 100000 records...\n",
      "---\n",
      "Processing complete!\n",
      "Final training data saved to: ./Data/training_data.jsonl\n",
      "Final validation data saved to: ./Data/validation_data.jsonl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "INPUT_JSONL_PATH = './Data/processed_chunks_with_qa.jsonl'\n",
    "TRAIN_OUTPUT_PATH = './Data/training_data.jsonl'\n",
    "VAL_OUTPUT_PATH = './Data/validation_data.jsonl'\n",
    "RETRIEVER_MODEL_NAME = 'all-MiniLM-L6-v2'\n",
    "VALIDATION_SPLIT_RATIO = 0.1 \n",
    "random.seed(42)\n",
    "\n",
    "# --- Load the Retriever Model ---\n",
    "print(f\"Loading retriever model: {RETRIEVER_MODEL_NAME}...\")\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "retriever_model = SentenceTransformer(RETRIEVER_MODEL_NAME, device=device)\n",
    "print(f\"Model loaded on device: {device}\")\n",
    "\n",
    "\n",
    "def find_best_chunk_for_record(record, model):\n",
    "    question = record.get('question')\n",
    "    chunks = record.get('chunks')\n",
    "    \n",
    "    if not chunks or not question:\n",
    "        return None\n",
    "\n",
    "    # Encode question and chunks into vector embeddings\n",
    "    question_embedding = model.encode(question, convert_to_tensor=True)\n",
    "    chunk_embeddings = model.encode(chunks, convert_to_tensor=True)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    cosine_scores = util.cos_sim(question_embedding, chunk_embeddings)\n",
    "\n",
    "    # Find the chunk with the highest score\n",
    "    best_chunk_index = cosine_scores.argmax()\n",
    "    best_chunk = chunks[best_chunk_index]\n",
    "\n",
    "    return {\n",
    "        'question': question,\n",
    "        'answer': record.get('answer'),\n",
    "        'context': best_chunk\n",
    "    }\n",
    "\n",
    "# --- Main Streaming, Processing, and Splitting Loop ---\n",
    "# This will read the large input file once and write to two output files.\n",
    "with open(INPUT_JSONL_PATH, 'r') as infile, \\\n",
    "     open(TRAIN_OUTPUT_PATH, 'w') as train_outfile, \\\n",
    "     open(VAL_OUTPUT_PATH, 'w') as val_outfile:\n",
    "\n",
    "    print(\"Starting retrieval and splitting into training and validation sets...\")\n",
    "\n",
    "    for i, line in enumerate(infile):\n",
    "        # Give a progress update every 5000 records\n",
    "        if (i + 1) % 5000 == 0:\n",
    "            print(f\"  - Processed {i+1} records...\")\n",
    "\n",
    "        # 1. Load the record\n",
    "        record = json.loads(line)\n",
    "        \n",
    "        # 2. Find the best chunk for this record\n",
    "        final_record = find_best_chunk_for_record(record, retriever_model)\n",
    "\n",
    "        # 3. Write the final record to the appropriate file\n",
    "        if final_record:\n",
    "            # Use a random roll to decide if this goes to validation or training\n",
    "            if random.random() < VALIDATION_SPLIT_RATIO:\n",
    "                val_outfile.write(json.dumps(final_record) + '\\n')\n",
    "            else:\n",
    "                train_outfile.write(json.dumps(final_record) + '\\n')\n",
    "\n",
    "print(\"---\")\n",
    "print(\"Processing complete!\")\n",
    "print(f\"Final training data saved to: {TRAIN_OUTPUT_PATH}\")\n",
    "print(f\"Final validation data saved to: {VAL_OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3660aa39",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9704ad6",
   "metadata": {},
   "source": [
    "## The Two Main Training Strategies\n",
    "There are two main ways to train a model like this:\n",
    "\n",
    "- \"Retrieve-then-Train\" (The Simpler, Pragmatic Approach):\n",
    "\n",
    "  - Process: This is the \"Step 2\" I proposed. We do the similarity search offline, one time, as part of building the final training set. For each question in our training data, we find the single \"best\" chunk from its context. We then create a clean, static dataset where every entry is a perfect (question, best_chunk, answer) triplet.\n",
    "  - Training: The model is then trained on these perfect examples. It learns one primary skill: \"Given a question and a highly relevant piece of context, extract or generate the answer.\"\n",
    "  - Inference: When a user asks a new question, you perform a live similarity search against your entire 13GB database of chunks to find the best one, and feed that to the model.\n",
    "\n",
    "- \"Joint\" or \"End-to-End\" Training (The Advanced RAG Approach):\n",
    "  - Process: This is a far more complex setup. During the training loop itself, for each question, you would dynamically perform a similarity search. You would retrieve not only the \"positive\" (correct) chunk but also several \"hard negative\" chunks (ones that are similar to the question but don't contain the answer).\n",
    "  - Training: The model is trained on this complex input (question, positive_chunk, negative_chunk_1, negative_chunk_2, ..., answer). It has to learn two skills simultaneously: 1) To pay more attention to the positive chunk and ignore the negatives, and 2) To generate the answer from the correct context. This trains both the \"retrieval\" and \"generation\" aspects of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4a81ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TriviaQADataset(Dataset):\n",
    "    def __init__(self, file_path: str, tokenizer: AutoTokenizer, max_length: int = 1024):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.records = []\n",
    "\n",
    "        # Load the data from the JSONL file\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                self.records.append(json.loads(line))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.records)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        record = self.records[idx]\n",
    "        context = record.get('context', '')\n",
    "        question = record.get('question', '')\n",
    "        answer = record.get('answer', '')\n",
    "\n",
    "        input_text = f\"Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer: {answer}\"\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            input_text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # For language modeling, the model predicts the next token, so labels are the input_ids.\n",
    "        # Squeeze to remove the batch dimension, as DataLoader will add it back.\n",
    "        input_ids = encoding['input_ids'].squeeze(0)\n",
    "        attention_mask = encoding['attention_mask'].squeeze(0)\n",
    "        \n",
    "        \n",
    "        labels = input_ids.clone()\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8abce96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "train_dataset = TriviaQADataset(\n",
    "    file_path='./Data/training_data.jsonl', \n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "validation_dataset = TriviaQADataset(\n",
    "    file_path='./Data/validation_data.jsonl', \n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529d2ef8",
   "metadata": {},
   "source": [
    "# LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bd73a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayer(torch.nn.Module):\n",
    "    def __init__(self, in_features, out_features, rank=16, alpha=16):\n",
    "        super().__init__()\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.scaling = self.alpha / self.rank\n",
    "\n",
    "        self.lora_A = torch.nn.Parameter(torch.zeros(rank, in_features))\n",
    "        self.lora_B = torch.nn.Parameter(torch.zeros(out_features, rank))\n",
    "        \n",
    "        # Initialize weights\n",
    "        torch.nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        lora_A_T = self.lora_A.T.to(dtype=x.dtype)\n",
    "        lora_B_T = self.lora_B.T.to(dtype=x.dtype)\n",
    "        \n",
    "        return (x @ lora_A_T @ lora_B_T) * self.scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0396902a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lora_to_model(model, rank=16, alpha=16, device='mps'):\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False  # Freeze original model parameters\n",
    "\n",
    "    linear_layers = []\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            linear_layers.append((name, module))\n",
    "\n",
    "    for name, module in linear_layers:\n",
    "            \n",
    "            lora_layer = LoRALayer(\n",
    "                in_features=module.in_features,\n",
    "                out_features=module.out_features,\n",
    "                rank=rank,\n",
    "                alpha=alpha\n",
    "            ).to(device)\n",
    "            \n",
    "            # Register the LoRA layer as a submodule of the linear layer\n",
    "            setattr(module, 'lora_layer', lora_layer)\n",
    "            \n",
    "            # Store original forward method\n",
    "            original_forward = module.forward\n",
    "            \n",
    "            # Define new forward method\n",
    "            def create_new_forward(original_fwd, lora_l):\n",
    "                def new_forward(x):\n",
    "                    return original_fwd(x) + lora_l(x) \n",
    "                return new_forward\n",
    "            \n",
    "            # Replace the forward method\n",
    "            module.forward = create_new_forward(original_forward, lora_layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c252921e",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf89150d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training parameter: lm_head.lora_layer.lora_A\n",
      "Training parameter: lm_head.lora_layer.lora_B\n",
      "\n",
      "Total parameters in model: 125256208\n",
      "Trainable LoRA parameters: 816400\n",
      "Percentage of trainable parameters: 0.6518%\n"
     ]
    }
   ],
   "source": [
    "lora_model = add_lora_to_model(model, device=device)\n",
    "\n",
    "\n",
    "lora_params = []\n",
    "for name, param in lora_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        lora_params.append(param)\n",
    "        print(f\"Training parameter: {name}\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(lora_params, lr=1e-4)\n",
    "\n",
    "total_params = sum(p.numel() for p in lora_model.parameters())\n",
    "trainable_params = sum(p.numel() for p in lora_params)\n",
    "\n",
    "print(f\"\\nTotal parameters in model: {total_params}\")\n",
    "print(f\"Trainable LoRA parameters: {trainable_params}\")\n",
    "print(f\"Percentage of trainable parameters: {100 * trainable_params / total_params:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d30138b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "740c6993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 1/3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/22707 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "Training:   0%|          | 58/22707 [01:25<9:15:35,  1.47s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     22\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 24\u001b[0m     total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m avg_train_loss \u001b[38;5;241m=\u001b[39m total_train_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_dataloader)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Average Training Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_train_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 3\n",
    "lora_model.to(device)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"--- Epoch {epoch + 1}/{NUM_EPOCHS} ---\")\n",
    "    \n",
    "    # --- Training ---\n",
    "    lora_model.train()\n",
    "    total_train_loss = 0\n",
    "    for batch in tqdm(train_dataloader, desc=\"Training\"):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Move batch to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = lora_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "        \n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1} | Average Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # --- Validation ---\n",
    "    lora_model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(validation_dataloader, desc=\"Validation\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = lora_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_val_loss += loss.item()\n",
    "            \n",
    "    avg_val_loss = total_val_loss / len(validation_dataloader)\n",
    "    print(f\"Epoch {epoch+1} | Average Validation Loss: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d3bb2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(model, tokenizer, question, context, device):\n",
    "    \"\"\"\n",
    "    Generates an answer given a question and a retrieved context.\n",
    "    \"\"\"\n",
    "    # Format the input for the model\n",
    "    prompt = f\"Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    "    \n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs.input_ids.to(device)\n",
    "    attention_mask = inputs.attention_mask.to(device)\n",
    "\n",
    "    print(f\"--- Generating answer for: '{question}' ---\")\n",
    "    \n",
    "    # Generate text using the model\n",
    "    output_sequences = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=50,  # Limit the length of the generated answer\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "    )\n",
    "    \n",
    "    # Decode the generated sequence\n",
    "    generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract just the answer part\n",
    "    # Handle cases where \"Answer:\" might not be present or is at the end\n",
    "    if \"Answer:\" in generated_text:\n",
    "        answer = generated_text.split(\"Answer:\")[1].strip()\n",
    "    else:\n",
    "        answer = generated_text.strip() # Fallback if \"Answer:\" isn't found\n",
    "    \n",
    "    print(f\"Generated Answer: {answer}\\n\")\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5435c16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model 'gpt2' and tokenizer on device: mps...\n",
      "Base model loaded.\n",
      "Applying LoRA structure to the model with rank=16, alpha=16...\n",
      "LoRA structure applied.\n",
      "Loading LoRA weights from 'lora_gpt2_triviaqa_weights.pth'...\n",
      "LoRA weights loaded successfully.\n",
      "\n",
      "--- Starting Evaluation on 10 Samples from Validation Set ---\n",
      "Context: One of the characteristics of the Cabindan independence movement is its constant fragmentation, into smaller and smaller factions. Economy\n",
      "\n",
      "Angola has a rich subsoil heritage, from diamonds, oil, gold, copper, and a rich wildlife (dramatically impoverished during the civil war), forest, and fossils. Since independence, oil and diamonds have been the most important economic resource. Smallholder and plantation agriculture have dramatically dropped because of the Angolan Civil War, but have begun to recover after 2002. The transformation industry that had come into existence in the late colonial period collapsed at independence, because of the exodus of most of the ethnic Portuguese population, but has begun to reemerge with updated technologies, partly because of the influx of new Portuguese entrepreneurs. Similar developments can be verified in the service sector. Overall, Angola's economy has in recent years moved on from the disarray caused by a quarter-century of civil war to become the fastest-growing economy in Africa and one of the fastest in the world, with an average GDP growth of 20 percent between 2005 and 2007. In the period 2001–10, Angola had the world's highest annual average GDP growth, at 11.1 percent. In 2004, the  Eximbank approved a $2 billion line of credit to Angola. The loan was to be used to rebuild Angola's infrastructure, and also to limited the influence of the International Monetary Fund in the country. China is Angola's biggest trade partner and export destination as well as the fourth-largest importer. Bilateral trade reached $27.67 billion in 2011, up 11.5% year-on-year. China's imports, mainly crude oil and diamonds, increased 9.1% to $24.89 billion while China's exports, including mechanical and electrical products, machinery parts and construction materials, surged 38.8%. The oil glut led to a local unleaded gasoline \"pricetag\" of £0.37 per gallon. The Economist reported in 2008 that diamonds and oil make up 60% of Angola's economy, almost all of the country's revenue and are its dominant exports. Growth is almost entirely driven by rising oil production which surpassed  in late 2005 and was expected to grow to 2 Moilbbl/d by 2007. Control of the oil industry is consolidated in Sonangol Group, a conglomerate owned by the Angolan government. In December 2006, Angola was admitted as a member of OPEC. However, operations in diamond mines include partnerships between state-run Endiama and mining companies such as ALROSA which continue operations in Angola. The economy grew 18% in 2005, 26% in 2006 and 17.6% in 2007. However, due to the global recession the economy contracted an estimated −0.3% in 2009. The security brought about by the 2002 peace settlement has led to the resettlement of 4 million displaced persons, thus resulting in large-scale increases in agriculture production. Although the country's economy has developed significantly since it achieved political stability in 2002, mainly thanks to the fast-rising earnings of the oil sector, Angola faces huge social and economic problems. These are in part a result of the almost continual state of conflict from 1961 onwards, although the highest level of destruction and socio-economic damage took place after the 1975 independence, during the long years of civil war. However, high poverty rates and blatant social inequality are chiefly the outcome of a combination of a persistent political authoritarianism, of \"neo-patrimonial\" practices at all levels of the political, administrative, military, and economic apparatuses, and of a pervasive corruption.\n",
      "\n",
      "Question: From which country did Angola achieve independence in 1975?\n",
      "\n",
      "Answer:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3z/9wzffpk50xx15ml6p58dl_hr0000gn/T/ipykernel_1338/3042043619.py:96: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  lora_state_dict = torch.load(LORA_WEIGHTS_PATH, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Sample #1\n",
      "Question: From which country did Angola achieve independence in 1975?\n",
      "Actual Answer:   Portugal\n",
      "Generated Answer: Angola\n",
      "----------------------------------------\n",
      "\n",
      "Context: Joan Alexandra Molinsky  (June 8, 1933 – September 4, 2014), better known as Joan Rivers, was an American comedian, actress, writer, producer, and television host noted for her often controversial comedic persona—where she was alternately self-deprecating or sharply acerbic, especially toward celebrities and politicians. Rivers came to prominence in 1965 as a guest on The Tonight Show. Hosted by her mentor, Johnny Carson, the show established Rivers' comedic style. In 1986, with her own rival program, The Late Show with Joan Rivers, Rivers became the first woman to host a late night network television talk show. She subsequently hosted The Joan Rivers Show (1989–1993), winning a Daytime Emmy for Outstanding Talk Show Host. Having become widely known for her comedic red carpet awards show celebrity interviews,   Rivers co-hosted the E! celebrity fashion show Fashion Police from 2010 to 2014 and starred in reality series Joan & Melissa: Joan Knows Best? (2011–2014) with daughter Melissa Rivers. She was the subject of the documentary Joan Rivers: A Piece of Work (2010). In addition to marketing a line of jewelry and apparel on the QVC shopping channel, Rivers authored 12 best-selling books and released numerous comedy albums. She was nominated in 1984 for a Grammy Award for her album What Becomes a Semi-Legend Most?; and was nominated in 1994 for the Tony Award for Best Actress in a Play for her performance of the title role in Sally Marr...and Her Escorts. In 2015, Rivers posthumously received a Grammy Award for Best Spoken Word Album for her book, Diary of a Mad Diva. In 1968, The New York Times television critic Jack Gould called Rivers \"quite possibly the most intuitively funny woman alive\". Early life\n",
      "\n",
      "Rivers was born Joan Alexandra Molinsky on June 8, 1933, in Brooklyn, New York,    the daughter of Russian Jewish immigrants Beatrice (née Grushman; January 6, 1906 – October 1975) and Dr. Molinsky (December 7, 1900 – January 1985), who graduated from Long Island College of Medicine. Her elder sister, Barbara Waxler, died on June 3, 2013 at the age of 82. Rivers was raised in Prospect Heights  and Crown Heights  in Brooklyn, where she attended the progressive and now-defunct Brooklyn Ethical Culture School and [http://blogs.wsj.com/bankruptcy/2014/06/17/brooklyn-prep-school-enters-chapter-11-bankruptcy/ Adelphi Academy of Brooklyn] - a college preparatory day school. Her family later moved to Larchmont. She attended Connecticut College between 1950 and 1952, and graduated from Barnard College in 1954 with a Bachelor of Arts degree in English literature and anthropology; she was a member of Phi Beta Kappa.Rivers, Joan (1986). Autobiography: Enter Talking. New York: Delacorte Press, First Printing. Before entering show business, Rivers worked at various jobs such as a tour guide at Rockefeller Center,Autobiography: Bouncing Back (1997), HarperCollins, pp. a writer/proofreader at an advertising agency and a fashion consultant at Bond Clothing Stores. During this period, agent Tony Rivers advised her to change her name, so she chose Joan Rivers as her stage name. Career\n",
      "\n",
      "1950s–1960s\n",
      "\n",
      "During the late 1950s, Rivers appeared in a short-run play, Driftwood, playing a lesbian with a crush on a character played by a then-unknown Barbra Streisand. The play ran for six weeks.\n",
      "\n",
      "Question: How is Joan Molinsky better known?\n",
      "\n",
      "Answer:\n",
      "----------------------------------------\n",
      "Sample #2\n",
      "Question: How is Joan Molinsky better known?\n",
      "Actual Answer:   Joan Rivers\n",
      "Generated Answer: She is best known in the United States as the mother of Joan and Joan's two children, who are now twins. Joan was married to a wealthy New Yorker named John Molinski in 1953. Their first child, a daughter, is named Joan.\n",
      "----------------------------------------\n",
      "\n",
      "Context: The Vought-Sikorsky VS-300 (or S-46) was a single-engine helicopter designed by Igor Sikorsky. It had a single three-blade rotor originally powered by a 75 horsepower (56 kW) engine. The first \"free\" flight of the VS-300 was on 13 May 1940. The VS-300 was the first successful single lifting rotor helicopter in the United States and the first successful helicopter to use a single vertical-plane tail rotor configuration for antitorque. With floats attached, it became the first practical amphibious helicopter. Design and development\n",
      "\n",
      "Igor Sikorsky's quest for a practical helicopter began in 1938, when as the Engineering Manager of the Vought-Sikorsky Division of United Aircraft Corporation, he was able to convince the directors of United Aircraft that his years of study and research into rotary-wing flight problems would lead to a breakthrough. His first experimental machine, the VS-300, was test flown by Sikorsky on 14 September 1939 tethered by cables. In developing the concept of rotary-wing flight, Sikorsky was the first to introduce a single engine to power both the main and tail rotor systems. The only previous successful attempt at a single-lift rotor helicopter, the Yuriev-Cheremukhin TsAGI-1EA in 1931 in the Soviet Union, used a pair of uprated, Russian-built Gnome Monosoupape rotary engines of 120 hp each for its power. For later flights of his VS-300, Sikorsky also added a vertical aerofoil surface to the end of the tail to assist anti-torque but this was later removed when it proved to be ineffective. The cyclic control was found to be difficult to perfect, and led to Sikorsky locking the cyclic and adding two smaller vertical-axis lifting rotors to either side aft of the tail boom. By varying pitch of these rotors simultaneously, fore and aft control was provided. Roll control was provided by differential pitching of the blades. In this configuration, it was found that the VS-300 could not fly forward easily and Sikorsky joked about turning the pilot's seat around. Operational history\n",
      "\n",
      "Sikorsky fitted utility floats (also called pontoons) to the VS-300 and performed a water landing and takeoff on 17 April 1941, making it the first practical amphibious helicopter.[http://www.sikorsky.com/vgn-ext-templating-SIK/v/index.jsp?vgnextoid\n",
      "208ae39d40a78110VgnVCM1000001382000aRCRD \"Timeline.\"] Sikorsky.com. Retrieved: 22 September 2009. On 6 May 1941, the VS-300 beat the world endurance record held by the Focke-Wulf Fw 61, by staying aloft for 1 hour 32 minutes and 26.1 seconds. The final variant of the VS-300 was powered by a 150 hp Franklin engine. The VS-300 was one of the first helicopters capable of carrying cargo. The VS-300 was modified over a two-year period, including removal of the two vertical tail rotors, until 1941 when a new cyclic control system gave it much improved flight behavior.Chiles 2008, p. Survivor\n",
      "\n",
      "In 1943, the VS-300 was retired to the Henry Ford Museum in Dearborn, Michigan. It has been on display there ever since, except for a trip back to the Sikorsky Aircraft plant for restoration in 1985. Specifications (VS-300)\n",
      "\n",
      "Question: The VS-300 was a type of what?\n",
      "\n",
      "Answer:\n",
      "----------------------------------------\n",
      "Sample #3\n",
      "Question: The VS-300 was a type of what?\n",
      "Actual Answer:   Helicopter\n",
      "Generated Answer: A single rotor rotor.\n",
      "*\n",
      "----------------------------------------\n",
      "\n",
      "Context: They believe the kingdom was established in heaven in 1914,  and that Jehovah's Witnesses serve as representatives of the kingdom on earth. Eschatology\n",
      "\n",
      "A central teaching of Jehovah's Witnesses is that the current world era, or \"system of things\", entered the \"last days\" in 1914 and faces imminent destruction through intervention by God and Jesus Christ, leading to deliverance for those who worship God acceptably. They consider all other present-day religions to be false, identifying them with \"Babylon the Great\", or the \"harlot\", of Revelation 17,  and believe that they will soon be destroyed by the United Nations, which they believe is represented in scripture by the scarlet-colored wild beast of Revelation chapter 17. This development will mark the beginning of the \"great tribulation\". Satan will subsequently attack Jehovah's Witnesses, an action that will prompt God to begin the war of Armageddon, during which all forms of government and all people not counted as Christ's \"sheep\", or true followers, will be destroyed. After Armageddon, God will extend his heavenly kingdom to include earth, which will be transformed into a paradise similar to the Garden of Eden. Most of those who had died before God's intervention will gradually be resurrected during \"judgment day\" lasting for one thousand years. This judgment will be based on their actions after resurrection rather than past deeds. At the end of the thousand years, Christ will hand all authority back to God. Then a final test will take place when Satan is released to mislead perfect mankind. Those who fail will be destroyed, along with Satan and his demons. The end result will be a fully tested, glorified human race. Jehovah's Witnesses believe that Jesus Christ began to rule in heaven as king of God's kingdom in October 1914, and that Satan was subsequently ousted from heaven to the earth, resulting in \"woe\" to humanity. They believe that Jesus rules invisibly, from heaven, perceived only as a series of \"signs\". They base this belief on a rendering of the Greek word parousia—usually translated as \"coming\" when referring to Christ—as \"presence\". They believe Jesus' presence includes an unknown period beginning with his inauguration as king in heaven in 1914, and ending when he comes to bring a final judgment against humans on earth. They thus depart from the mainstream Christian belief that the \"second coming\" of Matthew 24 refers to a single moment of arrival on earth to judge humans. Practices\n",
      "\n",
      "Worship\n",
      "\n",
      "Meetings for worship and study are held at Kingdom Halls, which are typically functional in character, and do not contain religious symbols. Witnesses are assigned to a congregation in whose \"territory\" they usually reside and attend weekly services they refer to as \"meetings\" as scheduled by congregation elders. The meetings are largely devoted to study of Watch Tower Society literature and the Bible. The format of the meetings is established by the religion's headquarters, and the subject matter for most meetings is the same worldwide. Congregations meet for two sessions each week comprising five distinct meetings that total about three-and-a-half hours, typically gathering mid-week (three meetings) and on the weekend (two meetings). Prior to 2009, congregations met three times each week; these meetings were condensed, with the intention that members dedicate an evening for \"family worship\". Gatherings are opened and closed with kingdom songs (hymns) and brief prayers. Twice each year, Witnesses from a number of congregations that form a \"circuit\" gather for a one-day assembly. Larger groups of congregations meet once a year for a three-day \"regional convention\", usually at rented stadiums or auditoriums.\n",
      "\n",
      "Question: When did the founder of Jehovah's Witnesses say the world would end?\n",
      "\n",
      "Answer:\n",
      "----------------------------------------\n",
      "Sample #4\n",
      "Question: When did the founder of Jehovah's Witnesses say the world would end?\n",
      "Actual Answer:   1914\n",
      "Generated Answer: Jehovah was not the first to say that. In the early days of Christianity, the word \"end\" was used to refer only to \"the end\" and not to any end. However, in the late 19th century, Jehovah began using the\n",
      "----------------------------------------\n",
      "\n",
      "Context: Censorship\n",
      "\n",
      "Perceived by the Catholic Church as a parody of the second coming of Jesus, the opening scene and the film as a whole were condemned by the Vatican newspaper L'Osservatore Romano in 1960. Subject to widespread censorship, the film was banned in Spain, until the death of Franco in 1975. Umberto Tupini, the Minister of Culture of the Tambroni government censored it and other \"shameful films\". Awards and recognition\n",
      "\n",
      "The New York Times hailed La dolce vita as \"one of the most widely seen and acclaimed European movies of the 1960s\". It was nominated for four Academy Awards, and won one for Best Costume Design: Black-and-White. La dolce vita also earned the Palme d'Or (Golden Palm) at the 1960 Cannes Film Festival. Entertainment Weekly voted it the 6th Greatest film of all time. In 2010, the film was ranked #11 in Empire magazine's \"The 100 Best Films Of World Cinema\". In popular culture\n",
      "\n",
      "As mentioned above, one of the characters (Paparazzo) is the inspiration for the popular term \"paparazzi,\" a word for an intrusive photojournalist. Tributes to Fellini in the \"Director's Cut\" of Cinema Paradiso (1988) include a helicopter suspending a statue of Christ over the city and scenes in which the Trevi Fountain is used as a backdrop while Toto, the main character, grows up to be a famous film director. Woody Allen's Celebrity (1998) is a New York-set re-working of La dolce vita with Kenneth Branagh taking up Mastroianni's role, and Winona Ryder and Charlize Theron taking on the roles held by Anouk Aimée and Anita Ekberg, respectively. In Sofia Coppola's film Lost in Translation (2003), Kelly's interview for LIT resembles Sylvia's interview scenes in La dolce vita. Charlotte and Bob later meet in the middle of the night and watch the famous Trevi Fountain sequence while drinking sake. Coppola said, \"I saw that movie on TV when I was in Japan. It's not plot-driven, it's about them wandering around. And there was something with the Japanese subtitles and them speaking Italian - it had a truly enchanting quality\". The 2013 Italian film The Great Beauty features a former writer who wanders through the parties of the Roman high society trying to decide what to do with his life.\n",
      "\n",
      "Question: Who directed the movie La Dolce Vita?\n",
      "\n",
      "Answer:\n",
      "----------------------------------------\n",
      "Sample #5\n",
      "Question: Who directed the movie La Dolce Vita?\n",
      "Actual Answer:   Federico Fellini\n",
      "Generated Answer: Leonardo DiCaprio\n",
      "----------------------------------------\n",
      "\n",
      "Context: When the roll is fully exposed, the take up spool is removed for processing and the empty spool on which the film was originally wound is moved to the other side, becoming the take up spool for the next roll of film. History\n",
      "\n",
      "In 1881 a farmer in Cambria, Wisconsin, Peter Houston, invented the first roll film camera. His younger brother David, filed the patents for various components of Peter's camera. David Henderson Houston (b. May 6, 1906\n",
      "), originally from Cambria, Wisconsin,  patented the first holders for flexible roll film. Houston moved to Hunter in Dakota Territory in 1880. He was issued an 1881 patent for a roll film holder    which he licensed to George Eastman (it was used in Eastman's Kodak 1888 box camera). Houston sold the patent (and an 1886 revision ) outright to Eastman for $5000 in 1889. Houston continued developing the camera, creating 21 patents for cameras or camera parts between 1881 and 1902. In 1912 his estate transferred the remainder of his patents to Eastman. The most popular rollfilm is the type 120 film format, which is used in most medium-format cameras and roll film magazines for large-format cameras. Until the 1950s, 120 roll film was also used in the then most simplist of snapshot cameras, and box cameras. The use of roll film in consumer cameras was largely superseded by 135 and 126 cartridges, but 120 and 220 (double length) film are still commonly used in medium-format cameras. A camera is an optical instrument for recording or capturing images, which may be stored locally, transmitted to another location, or both. The images may be individual still photographs or sequences of images constituting videos or movies.The camera is a remote sensing device as it senses subjects without physical contact. The word camera comes from camera obscura, which means \"dark chamber\" and is the Latin name of the original device for projecting an image of external reality onto a flat surface. The modern photographic camera evolved from the camera obscura. The functioning of the camera is very similar to the functioning of the human eye. Functional description\n",
      "\n",
      "A camera may work with the light of the visible spectrum or with other portions of the electromagnetic spectrum. A still camera is an optical device which creates a single image of an object or scene, and records it on an electronic sensor or photographic film. All cameras use the same basic design: light enters an enclosed box through a converging lens/convex lens and an image is recorded on a light-sensitive medium(mainly a transition metal-hallide). A shutter mechanism controls the length of time that light can enter the camera. Most photographic cameras have functions that allow a person to view the scene to be recorded, allow for a desired part of the scene to be in focus, and to control the exposure so that it is not too bright or too dim. A display, often a liquid crystal display (LCD), permits the user to view scene to be recorded and settings such as ISO speed, exposure, and shutter speed. A movie camera or a video camera operates similarly to a still camera, except it records a series of static images in rapid succession, commonly at a rate of 24 frames per second. When the images are combined and displayed in order, the illusion of motion is achieved. Thanks to the help of modern science, A group of photoscientists of MIT has successfully created a camera having frame rate of 1 trillion per second, able to see the light emerging and reflecting or refracting on an opaque or translucent media. History\n",
      "\n",
      "The forerunner to the photographic camera was the camera obscura.\n",
      "\n",
      "Question: Which George invented the Kodak roll-film camera?\n",
      "\n",
      "Answer:\n",
      "----------------------------------------\n",
      "Sample #6\n",
      "Question: Which George invented the Kodak roll-film camera?\n",
      "Actual Answer:   Eastman\n",
      "Generated Answer: George Houston\n",
      "----------------------------------------\n",
      "\n",
      "Context: Felix, however, seems utterly incapable of enjoying anything and only finds purpose in pointing out his own and other people's mistakes and foibles. Even when he tries to do so in a gentle and constructive way, his corrections and suggestions prove extremely annoying to those around him. Oscar, his closest friend, feels compelled to throw him out after only a brief time together, though he quickly realizes that Felix has had a positive effect on him. The play and the film both spell Felix's name Ungar, while the television series spells it Unger. Characters\n",
      "\n",
      "*Oscar Madison: A slovenly, recently divorced sportswriter. *Felix Ungar: A fastidious, hypochondriac photographer whose marriage is ending. *Murray: A NYPD policeman, one of Oscar and Felix's poker buddies. *Speed: One of the poker buddies. Gruff and sarcastic, often picking on Vinnie and Murray. *Vinnie: One of the poker buddies. Vinnie is mild-mannered and henpecked, making him an easy target for Speed's verbal barbs. *Roy: One of the poker buddies. Roy has a dry wit but is less acerbic than Speed. *Cecily and Gwendolyn Pigeon: Oscar and Felix's giggly upstairs neighbors, a pair of English sisters. The former is a divorcée, the latter a widow. Productions\n",
      "\n",
      "The Odd Couple premiered on Broadway at the Plymouth Theatre on March 10, 1965 and transferred to the Eugene O'Neill Theatre where it closed on July 2, 1967 after 964 performances and two previews. Directed by Mike Nichols, the cast starred Walter Matthau as Oscar Madison and Art Carney as Felix Ungar.[http://www.ibdb.com/production.php?id\n",
      "3230 The Odd Couple] Internet Broadway database, accessed April 12, 2012 The production gained Tony Awards for Walter Matthau, Best Actor (Play), Best Author (Play), Best Direction of a Play, and Best Scenic Design (Oliver Smith), and was nominated for Best Play. Matthau was replaced with Jack Klugman, starting in November 1965 and later Pat Hingle, starting in February 1966. Carney was replaced with Eddie Bracken starting in October 1965 and later Paul Dooley. Stage revivals\n",
      "\n",
      "In 1970, the McMaster Shakespearean Players performed The Odd Couple with Martin Short as Felix, Eugene Levy as Oscar, and Dave Thomas as Murray – before any of these performers were famous. In 1994, a version of the play moved to Glasgow and toured Scotland, starring Gerard Kelly as Felix, Craig Ferguson as Oscar and Kate Anthony as Gwendolyn Pigeon. Kelly reprised the role of Felix at the 2002 Edinburgh Fringe, opposite Andy Gray. In 1996, Klugman and Tony Randall reprised their roles from the TV series for a three-month run at the Theatre Royal in Haymarket, London. The production was an effort to raise money to support Randall's National Actors Theatre. (Klugman had previously played Oscar in London opposite Victor Spinetti as Felix.)\n",
      "\n",
      "In a 1997 issue of Premiere Magazine, Billy Crystal and Robin Williams announced a possible stage revival, in anticipation of success of their film Fathers' Day. When that film failed at the box office, the Crystal-Williams revival was quickly forgotten. Also in 1997, a tour of the US and Canada was mounted by Troupe America and Lake Pepin Players starring Jamie Farr as Oscar, William Christopher as Felix, and William Richard Rogers as Murray. The production was directed by Curt Wollan.\n",
      "\n",
      "Question: Which series had the characters Felix Unger and Oscar Madison?\n",
      "\n",
      "Answer:\n",
      "----------------------------------------\n",
      "Sample #7\n",
      "Question: Which series had the characters Felix Unger and Oscar Madison?\n",
      "Actual Answer:   The Odd Couple\n",
      "Generated Answer: The first two series were directed and written by Felix and Eugene Ungers. Felix was the first actor to play a character in the series, but Eugene was not the only actor. Eugene's character, Felix (played by Eugene), was a young man\n",
      "----------------------------------------\n",
      "\n",
      "Context: This would be the only time all three of the world's largest liners would be berthed together. Captain Townley received two telegrams on his arrival in New York, one from his wife congratulating him and the other from the ship's namesake – Her Majesty Queen Elizabeth, who thanked him for safe delivery of the ship that was named for her. The ship was then moored for the first time alongside Queen Mary and was secured so that no one could board her without prior permission. This included port officials. Cunard later issued a statement that it had been decided that, due to the global circumstances, it was best that the new liner was moved to a neutral location and that during that voyage the ship had carried no passengers or cargo. World War II\n",
      "\n",
      "Queen Elizabeth left the port of New York on 13 November 1940 for Singapore to receive her troopship conversion. After two stops to refuel and replenish her stores in Trinidad and Cape Town, she arrived in Singapore's Naval Docks where she was fitted with anti-aircraft guns, and her hull repainted black, although her superstructure remained grey. As a troopship, Queen Elizabeth left Singapore on 11 February, and initially she carried Australian troops to operating theatres in Asia and Africa. After 1942, the two Queens were relocated to the North Atlantic for the transportation of American troops to Europe. Queen Elizabeth and Queen Mary were used as troop transports during the war. Their high speeds allowed them to outrun hazards, principally German U-boats, usually allowing them to travel without a convoy. During her war service as a troopship Queen Elizabeth carried more than 750,000 troops, and she also sailed some . Her captains during this period were Townley, Ernest Fall, Cyril Gordon Illinsworth, Charles Ford, and James Bisset. Post-war career\n",
      "\n",
      "Following the end of World War II, her sister ship Queen Mary remained in her wartime role and grey appearance, except for her funnels, which were repainted in the company's colours. For another year she did military service, returning troops and G.I. brides to the United States. Queen Elizabeth, meanwhile, was refitted and furnished as an ocean liner at the Firth of Clyde Drydock in Greenock by the John Brown Shipyard. Six years of war service had never permitted the formal sea trials to take place, and these were now finally undertaken. Under the command of Commodore Sir James Bisset the ship travelled to the Isle of Arran and her trials were carried out. Onboard was the ship's namesake Queen Elizabeth and her two daughters, the princesses Elizabeth and Margaret. During the trials, her majesty Queen Elizabeth took the wheel for a brief time and the two young princesses recorded the two measured runs with stopwatches that they had been given for the occasion. Bisset was under strict instructions from Sir Percy Bates, who was also aboard the trials, that all that was required from the ship was two measured runs of no more than thirty knots and that she was not permitted to attempt to attain a higher speed record than Queen Mary. After her trials Queen Elizabeth finally entered Cunard White Star's two ship weekly service to New York. Despite similar specifications to her older sister ship Queen Mary, Elizabeth never held the Blue Riband, as Cunard White Star chairman Sir Percy Bates requested that the two ships not try to compete against one another. In 1955 during an annual overhaul at Southampton, England, Queen Elizabeth was fitted with underwater fin stabilizers to smooth the ride in rough seas. Two fins were fitted on each side of the hull. The fins were retractable into the hull to save fuel in smooth seas and for docking.\n",
      "\n",
      "Question: The Queen Elizabeth liner was destroyed by fire in the 70s in which harbour?\n",
      "\n",
      "Answer:\n",
      "----------------------------------------\n",
      "Sample #8\n",
      "Question: The Queen Elizabeth liner was destroyed by fire in the 70s in which harbour?\n",
      "Actual Answer:   Hong Kong\n",
      "Generated Answer: Queen Victoria\n",
      "----------------------------------------\n",
      "\n",
      "Context: Collins performed the soundtrack to the animated film Tarzan (1999) for The Walt Disney Company. He won an Academy Award for You'll Be in My Heart, which he performed at that year's telecast as well as during a Disney-themed Super Bowl halftime show. The song, which he also recorded in Spanish among other languages, became his only appearance on the Billboard Hot Latin Tracks chart. Disney hired Collins and Tina Turner for the soundtrack to the 2003 animated film, Brother Bear, and had some airplay with the song \"Look Through My Eyes\". Collins's music is featured in the satirical black comedy film American Psycho, with psychotic lead character Patrick Bateman (played by Christian Bale) portrayed as an obsessive fan who reads deep meaning into his work, especially with Genesis, while describing his solo music as \"...more commercial and therefore more satisfying, in a narrower way.\" Bateman delivers a monologue praising Collins and Genesis during a sequence in which he engages the services of two prostitutes while playing \"In Too Deep\" and \"Sussudio\". Collins twice hosted the Billboard Music Awards on television, which were produced and directed by his longtime music video and TV special collaborators, Paul Flattery and Jim Yukich of FYI (Flattery Yukich Inc). He also appeared in an episode of the series Miami Vice, entitled \"Phil the Shill\", in which he plays a cheating con-man. He also appeared in several sketches with The Two Ronnies. In 2001, Collins was one of several celebrities who were tricked into appearing in a controversial British comedy series, Brass Eye, shown on public service broadcaster Channel 4. In the episode, Collins endorsed a hoax anti-paedophile campaign wearing a T-shirt with the words \"Nonce Sense\" and warned children against speaking to suspicious people. Collins was reported by the BBC to have consulted lawyers regarding the programme, which was originally pulled from broadcast but eventually rescheduled. Collins said he had taken part in the programme \"in good faith for the public benefit\", believing it to be \"a public service programme that would be going around schools and colleges in a bid to stem child abduction and abuse\". Collins also accused the makers of the programme of \"some serious taste problems\" and warned it would prevent celebrities from supporting \"public spirited causes\" in the future. Collins appeared as himself in the 2006 PSP and PS2 video game Grand Theft Auto: Vice City Stories. Set in 1984, he appears in three missions in which the main character, Victor, must save him from a gang that is trying to kill him, the final mission occurring during his concert, where the player must defend the scaffolding against saboteurs while Collins is performing \"In the Air Tonight\". After this, the player is given the opportunity to watch this performance of \"In the Air Tonight\" for only 6,000 dollars in the game. \"In the Air Tonight\" was also featured in the soundtrack of Grand Theft Auto: Vice City Stories and it was also featured in the film Aqua Teen Hunger Force Colon Movie Film For Theaters, the 2009 movie The Hangover and the 2007 Gorilla commercial for Cadbury's Dairy Milk chocolate. The advertisement also helped the song re-enter the New Zealand RIANZ Singles Chart at No. 3 in July 2008, the following week reaching No. 1, beating its original 1981 No. \"In the Air Tonight\" was also sampled in the song \"I Can Feel It\" on Sean Kingston's self-titled debut album. Collins was portrayed in the cartoon South Park in the episode \"Timmy 2000\" holding his Oscar throughout, referring to his 1999 win for You'll Be in My Heart, which defeated \"Blame Canada\" from South Park: Bigger, Longer & Uncut.\n",
      "\n",
      "Question: Phil Collins appeared in which Spielberg film with Robin Williams?\n",
      "\n",
      "Answer:\n",
      "----------------------------------------\n",
      "Sample #9\n",
      "Question: Phil Collins appeared in which Spielberg film with Robin Williams?\n",
      "Actual Answer:   Hook\n",
      "Generated Answer: In which film?\n",
      "----------------------------------------\n",
      "\n",
      "Context: Jacqueline Lee \"Jackie\" Kennedy Onassis (née Bouvier, pronounced; July 28, 1929 – May 19, 1994) was the wife of the 35th President of the United States, John F. Kennedy, and First Lady of the United States during his presidency from 1961 until his assassination in 1963. Bouvier was the elder daughter of Wall Street stockbroker John Vernou Bouvier III and socialite Janet Lee Bouvier. In 1951, she graduated with a Bachelor of Arts degree in French literature from George Washington University and went on to work for the Washington Times-Herald as an inquiring photographer. In 1952, Bouvier met Congressman John F. Kennedy at a dinner party. Shortly after, he was elected to the United States Senate and the couple married the following year. They had four children, two of whom died in infancy. As First Lady, she aided her husband's administration with her presence in social events and with her highly publicized restoration of the White House. On November 22, 1963, she was riding with him in a motorcade in Dallas, Texas, when he was assassinated. She and her children withdrew from public view after his funeral, and she married Aristotle Onassis in 1968. Following her second husband's death in 1975, she had a career as a book editor for the final two decades of her life. She is remembered for her contributions to the arts and preservation of historic architecture, as well as for her style, elegance, and grace. She was a fashion icon; her famous ensemble of pink Chanel suit and matching pillbox hat has become symbolic of her husband's assassination and one of the most iconic images of the 1960s. She ranks as one of the most popular First Ladies and in 1999 was named on Gallup's list of Most Admired Men and Women in 20th century America. Early life (1929–1951) \n",
      "\n",
      "Family and childhood\n",
      "\n",
      "Jacqueline Lee Bouvier was born on July 28, 1929 at Southampton Hospital in Southampton, New York, to Wall Street stockbroker John Vernou \"Black Jack\" Bouvier III (1891–1957) and socialite Janet Norton Lee (1907–1989). Bouvier's mother was of Irish ancestry, and her father's ancestry included French, Scottish, and English. Named after her father, Bouvier was baptized at the Church of St. Ignatius Loyola in Manhattan; she was raised in the Catholic faith. Her younger sister Lee was born in 1933. Bouvier spent her early childhood years in Manhattan and at Lasata, the Bouviers' country estate in East Hampton on Long Island. She idolized her father, who likewise favored her over her sister, calling his eldest child \"the most beautiful daughter a man ever had\".Leaming (2014), pp. Biographer Tina Flaherty attributes her father's praise to fueling Bouvier's confidence in herself, and her sister Lee has stated that she would not have gained her \"independence and individuality\" had it not been for the relationship she had with their father and paternal grandfather.Tracy, pp. From an early age, Bouvier was an enthusiastic equestrienne and successfully competed in the sport; horse-riding would remain a lifelong passion. She also took ballet lessons, was an avid reader, and excelled at learning languages, with French being particularly emphasized in her upbringing. Bouvier was enrolled in the Chapin School in Manhattan in 1935, which she attended for grades 1–6. While a bright student, she often misbehaved; one of her teachers described her as \"a darling child, the prettiest little girl, very clever, very artistic, and full of the devil\".\n",
      "\n",
      "Question: Which newspaper did Jackie Kennedy work for just before her marriage?\n",
      "\n",
      "Answer:\n",
      "----------------------------------------\n",
      "Sample #10\n",
      "Question: Which newspaper did Jackie Kennedy work for just before her marriage?\n",
      "Actual Answer:   Washington Times Herald\n",
      "Generated Answer: The New Yorker, July 27, 1961.\n",
      "*\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_on_validation_data(model, tokenizer, validation_file_path, device, num_samples=10):\n",
    "    \"\"\"\n",
    "    Loads validation data, generates answers, and compares them to the actual answers.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Starting Evaluation on {num_samples} Samples from Validation Set ---\")\n",
    "    \n",
    "    try:\n",
    "        with open(validation_file_path, 'r') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i >= num_samples:\n",
    "                    break\n",
    "\n",
    "                record = json.loads(line)\n",
    "                question = record.get('question', '')\n",
    "                context = record.get('context', '')\n",
    "                actual_answer = record.get('answer', '')\n",
    "\n",
    "                # Format the input for the model\n",
    "                prompt = f\"Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    "                \n",
    "                # Tokenize the input\n",
    "                inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True) # Truncate long contexts\n",
    "\n",
    "\n",
    "                print(tokenizer.decode(inputs.input_ids[0], skip_special_tokens=True))\n",
    "\n",
    "\n",
    "                input_ids = inputs.input_ids.to(device)\n",
    "                attention_mask = inputs.attention_mask.to(device)\n",
    "                \n",
    "                # Generate text using the model\n",
    "                output_sequences = model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    max_new_tokens=50,\n",
    "                    no_repeat_ngram_size=2,\n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                    temperature=0.1, # Lower temperature for more deterministic, factual answers\n",
    "                    top_k=40,\n",
    "                )\n",
    "                \n",
    "                # Decode the generated sequence\n",
    "                generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "                \n",
    "                # Extract just the answer part\n",
    "                if \"Answer:\" in generated_text:\n",
    "                    generated_answer = generated_text.split(\"Answer:\")[1].strip()\n",
    "                else:\n",
    "                    generated_answer = \"Could not parse answer from model output.\"\n",
    "\n",
    "                # Print the comparison\n",
    "                print(\"-\" * 40)\n",
    "                print(f\"Sample #{i+1}\")\n",
    "                print(f\"Question: {question}\")\n",
    "                print(f\"Actual Answer:   {actual_answer}\")\n",
    "                print(f\"Generated Answer: {generated_answer}\")\n",
    "                print(\"-\" * 40 + \"\\n\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Validation file not found at '{validation_file_path}'.\")\n",
    "        print(\"Please ensure the path is correct.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during evaluation: {e}\")\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "\n",
    "# Configuration\n",
    "MODEL_NAME = 'gpt2'\n",
    "LORA_WEIGHTS_PATH = 'lora_gpt2_triviaqa_weights.pth'\n",
    "VALIDATION_DATA_PATH = './Data/validation_data.jsonl'\n",
    "NUM_SAMPLES_TO_EVALUATE = 10 # Adjust as needed\n",
    "\n",
    "# 1. Load Model and Tokenizer\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Loading base model '{MODEL_NAME}' and tokenizer on device: {device}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token \n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "model.to(device)\n",
    "print(\"Base model loaded.\")\n",
    "\n",
    "# 2. Apply LoRA structure to the model\n",
    "lora_rank = 16\n",
    "lora_alpha = 16\n",
    "print(f\"Applying LoRA structure to the model with rank={lora_rank}, alpha={lora_alpha}...\")\n",
    "model = add_lora_to_model(model, rank=lora_rank, alpha=lora_alpha, device=device)\n",
    "print(\"LoRA structure applied.\")\n",
    "\n",
    "# 3. Load the fine-tuned LoRA weights\n",
    "print(f\"Loading LoRA weights from '{LORA_WEIGHTS_PATH}'...\")\n",
    "\n",
    "lora_state_dict = torch.load(LORA_WEIGHTS_PATH, map_location=device)\n",
    "model.load_state_dict(lora_state_dict, strict=False)\n",
    "print(\"LoRA weights loaded successfully.\")\n",
    "\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# 4. Run evaluation\n",
    "evaluate_on_validation_data(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    validation_file_path=VALIDATION_DATA_PATH,\n",
    "    device=device,\n",
    "    num_samples=NUM_SAMPLES_TO_EVALUATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8612a02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5106c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
